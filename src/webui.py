import gradio as gr
from config import *
from prompt_template import *
from langchain.document_loaders import UnstructuredExcelLoader
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.docstore.document import Document
from langchain.vectorstores.pgvector import PGVector


# 1. åŠ è½½ç›¸å…³å‚æ•°
# embedding = op_embedding
embedding = hf_embedding
# PGVector æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
CONNECTION_STRING = CONNECTION_STRING
# é›†åˆåç§°
COLLECTION_NAME = COLLECTION_NAME
# æ„å»º Prompt æ¨¡æ¿
QA_CHAIN_PROMPT = QA_CHAIN_PROMPT


# 2. è¿æ¥å·²æœ‰çš„å‘é‡æ•°æ®åº“
def conn_vectorstore():
    store = PGVector(
        collection_name=COLLECTION_NAME,
        connection_string=CONNECTION_STRING,
        embedding_function=embedding,
    )
    return store

db = conn_vectorstore()


# 3. è°ƒç”¨ LLM ç”Ÿæˆè¾“å‡º
def llm_predict(question, top_k, temperature, chat_history):
    # åŠ è½½æ¨¡å‹
    llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=temperature) 
    
    # ä½¿ç”¨çŸ¢é‡æ•°æ®åº“ä½œä¸ºæ£€ç´¢å™¨
    retriever = db.as_retriever(
        # è·å– 50 ä¸ªç›¸å…³æ–‡æ¡£ä»¥ä¾›ç›¸ä¼¼æ€§ç®—æ³•è€ƒè™‘, ä½†åªè¿”å›å‰ k ä¸ª, MMR ç®—æ³•å¥½åƒæœ‰é—®é¢˜
        search_kwargs={'k': top_k, 'fetch_k': 50}
        # search_type="mmr",
        # search_kwargs={'k': top_k}
    )
    # ç”Ÿæˆ QA é—®ç­”é“¾
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )
    result = qa_chain({"query": question})
    chat_history.append((question, result["result"]))
    # é¢„ç•™ä¸€ä¸ªç©ºå­—ç¬¦ä¸²çš„è¾“å‡ºå­˜æ”¾é—®é¢˜çš„æ‰“å°
    return "", chat_history


# 4. å‘å·²æœ‰å‘é‡æ•°æ®åº“ä¸­æ·»åŠ æ–°çš„æ–‡æœ¬æ•°æ®
def add_files():
    db.add_documents([Document(page_content="foo")])


# 5. ä½¿ç”¨ gradio æ„å»ºå¯è§†åŒ–æ¨¡å—
with gr.Blocks() as demo:
    gr.Markdown(
        """
        # <center>è®ºæ–‡è‡ªåŠ¨åŒ–åˆ†æå·¥å…· v1.0</center>
        """
    )

    with gr.Row():
        with gr.Column(scale=1):
            model_argument = gr.Accordion("æ¨¡å‹å‚æ•°é…ç½®")
            with model_argument:
                top_k = gr.Slider(1,
                                    10,
                                    value=6,
                                    step=1,
                                    label="æœ€ç›¸å…³å‘é‡æ•°",
                                    interactive=True)

                history_len = gr.Slider(0,
                                        5,
                                        value=3,
                                        step=1,
                                        label="ä¸Šä¸‹æ–‡é•¿åº¦",
                                        interactive=True)

                temperature = gr.Slider(0,
                                        1,
                                        value=0.70,
                                        step=0.01,
                                        label="å›ç­”çµæ´»åº¦",
                                        interactive=True)

            file = gr.File(label='è¯·ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶',
                            file_types=['.txt', '.md', '.docx', '.pdf'])

            init_vs = gr.Button("çŸ¥è¯†åº“æ–‡ä»¶å‘é‡åŒ–")

            gr.Markdown("""æé†’ï¼š<br>
                1. ä½¿ç”¨æ—¶è¯·å…ˆä¸Šä¼ è‡ªå·±çš„çŸ¥è¯†æ–‡ä»¶ï¼Œå¹¶ä¸”æ–‡ä»¶ä¸­ä¸å«æŸäº›ç‰¹æ®Šå­—ç¬¦ï¼Œå¦åˆ™å°†è¿”å›error. <br>
                2. ä¹Ÿå¯ä»¥åŠ è½½æœ¬åœ°å·²æœ‰çš„çŸ¥è¯†åº“ã€‚
                """)

        with gr.Column(scale=3):
            chatbot = gr.Chatbot([], label='Chatbot', height=500)
            message = gr.Textbox(label='è¯·è¾“å…¥ç›¸å…³é—®é¢˜')
            state = gr.State()

            with gr.Row():
                clear_history = gr.ClearButton([message, chatbot], value="ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                send = gr.Button("ğŸš€ å‘é€")
            
            gr.Examples(["è¯·ä»‹ç»ä¸€ä¸‹ GPP", "è¯·ä»‹ç»ä¸€ä¸‹ SIF", "SIFä¸GPPæœ‰ä»€ä¹ˆå…³ç³»?"], message, cache_examples=False)

        # å›è½¦äº‹ä»¶
        message.submit(fn=llm_predict, inputs=[message, top_k, temperature, chatbot], outputs=[message, chatbot])
        # å•å‡»äº‹ä»¶
        send.click(fn=llm_predict, 
                   inputs=[message, top_k, temperature, chatbot],
                   outputs=[message, chatbot],
                   api_name="llm_predict")
        
        # init_vs.click(fn=add_files)


if __name__ == "__main__":
    # åœ¨åŒä¸€å±€åŸŸç½‘ç»œä¸­å¯ä»¥è®¿é—®
    demo.launch(server_name='0.0.0.0', # æœ¬åœ°è®¾ç½®ä¸º 127.0.0.1
                server_port=7860,
                share=True # æœ¬åœ°è®¾ä¸º False
                ) 

# gr.close_all() # å…³é—­æ‰€æœ‰ gradio ç›¸å…³ç«¯å£
# 